# 开发文档：基于 **Python + PyQt + LLM** 的批量/单文件翻译工具

# 1 目标与范围

**核心功能**

* 支持语言对选择（例如：中文→英文、英文→中文，可扩展）
* 支持选择单个文件或本地文件夹（递归）
* 可设置输出路径（保留原结构或合并输出）
* 支持分块翻译（防止超 token）
* UI 分区展示功能（分选项卡/模块化）
* 可配置并保存 LLM 提示词（Prompt）
* 清晰的代码层次（UI / 业务 / 存储 / LLM）

---

# 2 总体架构（分层）

```
app/
├─ ui/                    # PyQt 界面文件（.py 或 .ui）
│  ├─ main_window.py
│  ├─ widgets/
│  └─ resources/
├─ core/                  # 业务逻辑
│  ├─ file_manager.py     # 文件遍历、读取、写入、输出路径管理
│  ├─ chunker.py          # 分块逻辑、token 估算
│  ├─ translator.py       # 翻译流程编排（调用 LLM）
│  └─ prompt_manager.py   # prompt 存取与管理
├─ llm/                   # LLM 适配层（封装 API）
│  ├─ client_base.py
│  ├─ openai_client.py
│  └─ local_llm_client.py # 若部署本地 LLM
├─ config/                # 配置文件（yaml/json）
│  └─ default_config.yaml
├─ tests/                 # 单元测试与集成测试
└─ app.py                 # 程序入口
```

每层职责清晰：UI 只关心与用户交互，core 处理业务流程，llm 层负责与模型的通讯（把第三方 API 都封装进去，方便替换）。

---

# 3 界面设计（PyQt）

建议使用 **主窗口 - 选项卡** 的方式，把功能分区显示（满足“不要一股脑堆在一起”）：

主窗口 (MainWindow) 分三大 Tab：

1. **源/目标设置（Language）**

   * 下拉：源语言、目标语言（可扩展）
   * 语言检测开关（自动检测源语言）
2. **文件选择（Files）**

   * 单文件选择按钮（FileDialog）
   * 文件夹选择按钮（FolderDialog），带递归/非递归选择项
   * 文件列表（表格）：列显示文件名、大小、状态（待翻/已翻/错误）
3. **翻译设置（Settings）**

   * 输出路径选择（保存原目录结构 / 指定目录）
   * 分块策略：

     * 自动（基于 token 限制）
     * 手动：最大字符数 / 最大 token 数 / 固定段落拆分
   * 并发数（1..N）
   * 保存/加载 Prompt 模板（下拉选择 + 编辑器）
   * LLM 配置（选择模型、API Key 管理、超时/重试）
4. **运行与日志（Run / Log）**

   * 开始/暂停/取消按钮
   * 进度条、单文件进度
   * 实时日志输出窗口（可导出）
5. **高级（可选）**

   * 翻译结果比对（原文/译文并排）
   * 翻译质量评分（基本）

UI 交互原则：所有耗时操作通过后台线程（QThread 或 concurrent.futures）执行，并以信号槽更新 UI。

---

# 4 核心模块设计与关键算法

## 4.1 文件管理（file_manager.py）

职责：

* 接受单文件或文件夹路径
* 根据扩展名选择解析/保存策略（txt, md, docx, html 等）
* 提供读取纯文本接口（返回字符串或段落列表）
* 提供写入接口（保持编码与原文件名映射）
* 支持“保留原目录结构”到指定输出根目录

接口示例：

```python
class FileManager:
    def list_files(path: str, recursive: bool=True) -> List[Path]
    def read_file(path: Path) -> str  # 返回文本
    def write_file(output_root: Path, original_path: Path, content: str) -> Path
```

注意：对于 DOCX/PDF 需要用专门库（python-docx, pdfminer.six/fitz）解析与写回；初版可以先支持 txt/md。

## 4.2 分块器（chunker.py）

目标：把大文本分成安全的块，避免超 token，并尽量保持语义完整（尽量按段落/句子分）。

策略（优先级）：

1. 按段落（双换行分段）
2. 若段落过长，按句子（使用简单的句子切分或 nltk/split by punctuation）
3. 基于 token 估算控制最大 token（由模型 token_limit 决定）

需要实现：

* `estimate_tokens(text: str) -> int`：估算 token 数（若使用 OpenAI，可使用 tiktoken；初版可用 chars_to_tokens = len(text) / 4 作为近似）
* `chunk_text(text, max_tokens, prefer_boundary=["paragraph","sentence"]) -> List[str]`

示例伪代码：

```python
def chunk_text(text, max_tokens):
    paragraphs = text.split('\n\n')
    chunks = []
    current = ""
    for p in paragraphs:
        if estimate_tokens(current + p) <= max_tokens:
            current += "\n\n" + p
        else:
            if current:
                chunks.append(current)
            # if single paragraph too big -> split by sentence
            if estimate_tokens(p) > max_tokens:
                for s in split_sentences(p):
                    if estimate_tokens(current + s) <= max_tokens:
                        current += s
                    else:
                        if current: chunks.append(current)
                        current = s
            else:
                current = p
    if current: chunks.append(current)
    return chunks
```

## 4.3 LLM 层（llm/client_base.py, openai_client.py）

职责：

* 抽象出 `translate_text(prompt, text, model, max_tokens, timeout)` 等通用方法
* 管理 API Key、重试、超时与速率限制（可配置）

建议接口：

```python
class LLMClientBase:
    def translate(self, prompt_template: str, source_lang: str, target_lang: str, text: str) -> str
```

**注意**：将 prompt 与 text 一并传入，不在 UI 层拼接细节。把 prompt 模板保存在 config 或数据库（本地 json/yaml）。

示例（OpenAI）伪代码：

```python
response = openai.ChatCompletion.create(
    model=model,
    messages=[
        {"role":"system","content": system_prompt},
        {"role":"user","content": user_prompt.format(src=src, tgt=tgt, text=text)}
    ],
    max_tokens=...,
    temperature=0.0
)
```

## 4.4 翻译编排（translator.py）

职责：

* 接受文件 -> read -> chunk -> translate each chunk -> 合并 -> write
* 支持并发（但保留单文件顺序）
* 提供进度回调/事件（供 UI 更新）
* 错误处理：每个 chunk 单独重试 N 次，失败后记录并继续

流程图（简化）：

```
for file in files:
  text = file_manager.read_file(file)
  chunks = chunker.chunk_text(text, max_tokens)
  for chunk_idx, chunk in enumerate(chunks):
    translated_chunk = llm_client.translate(prompt, src, tgt, chunk)
    collect translated_chunk
  merged = merge_chunks(translated_chunks)
  file_manager.write_file(output_root, file, merged)
  emit file_done
```

---

# 5 Prompt（提示词）管理

* 提示词模板支持变量：`{source_lang}`, `{target_lang}`, `{text}`, `{instructions}`
* 提供 UI 编辑器（多行文本），保存为模板（json/yaml）
* 默认提示词示例（用于翻译保留格式、保留代码块）：

```
You are a professional translator. Translate the following text from {source_lang} to {target_lang}. Preserve formatting, code blocks, markdown structure, and lists. Keep translation faithful and concise. Text: {text}
```

* 支持“后处理规则”（如：是否保留 XML 标签、是否忽略特定正则段落）

---

# 6 Token 估算与分块参数建议

* **估算**：粗略估算：`tokens ≈ ceil(len(text_chars) / 4)`（一般英语每 4 字符 ~1 token），更准确可集成 tiktoken（若用 OpenAI）
* **默认参数**：

  * model max tokens: 4096（示例）
  * 留 margin：`usable = model_max_tokens - 512` 用于 prompt + response
  * chunk_max_token = min(user_setting, usable // 2)（留出生成空间）
* **配置项**（UI）：

  * 最大输入 token（或字符数）
  * 每块最大字符数（备用）
  * 是否在句子/段落边界拆分（优先保持句子完整）

---

# 7 并发、速率与重试

* 并发层次建议：

  * **文件级并发**：同时翻译多个文件 （线程池大小可配置）
  * **chunk 顺序性保证**：单文件内部 chunk 顺序必须保持合并顺序
* 使用 `concurrent.futures.ThreadPoolExecutor` 或 `QThreadPool` 执行
* 对 API 错误（Timeout, RateLimit）做指数回退重试（最多 3 次）
* 日志记录失败 chunk 和原因，支持人工重试某些文件

---

# 8 日志、进度与状态保存

* 每个文件持有状态：`{pending, running, success, failed}`
* 对于长任务，支持中断与恢复（保存作业元数据到本地 JSON：已完成哪些文件/chunks）
* 日志层（简单）：使用 Python `logging`，并在 UI 里显示 tail（按信号更新）

---

# 9 配置管理

* 使用 YAML/JSON 存放用户配置（位于 `~/.my_translator/config.yaml`）
* 配置示例字段：

```yaml
llm:
  provider: openai
  model: gpt-4o-mini
  api_key_name: default
translate:
  chunk_max_chars: 2000
  chunk_max_tokens: 1000
ui:
  last_opened_dir: /home/user/
prompts:
  default_prompt: "..."
```

* 提供导入/导出配置功能（便于团队共享）

---

# 10 错误处理策略

* 文件读取失败 -> 标记并跳过（记录原因）
* 单个 chunk 翻译失败 -> 重试 N 次 -> 若仍失败，可：

  * 标记文件为“部分失败”，写入已翻译部分并生成错误报告；或
  * 回滚（不写入）并等待手动重试（可配置）
* LLM 身份验证/配额不足 -> 提示用户并停止任务

---

# 11 测试策略

* 单元测试：

  * chunker 的各种文本样例（长段落/短句/混合）
  * file_manager 的读写（虚拟 temp 文件）
  * prompt_manager 的存取
* 集成测试：

  * 使用 mock LLM client（返回已知翻译）测试整个翻译流程
  * 测试并发与中断恢复
* UI 测试（手工或使用 Qt 测试框架）
* 用例（必测）：

  * 单文件 txt 翻译
  * 文件夹内多文件并发翻译
  * 超长文本按 chunk 切分并合并结果

---

# 12 安全与隐私

* API Key 本地加密存储（例如使用系统 Keyring 或本地加密文件）
* 明确提示用户上传文本到第三方 LLM 的隐私风险
* 可选：支持本地模型（local_llm_client）以避免外传

---

# 13 部署打包

* 使用 `PyInstaller` 打包为可执行文件（注意要包含依赖）
* 提供跨平台（Windows/macOS/Linux）打包脚本
* Release 包含：可执行、默认配置、README、示例 prompt

---
